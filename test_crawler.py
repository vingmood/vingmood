#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®åšçƒ­æœæŠ“å–è„šæœ¬æµ‹è¯•
"""

import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from weibo_hot_search import WeiboHotSearchCrawler

def test_crawler():
    """æµ‹è¯•çˆ¬è™«åŠŸèƒ½"""
    print("å¼€å§‹æµ‹è¯•å¾®åšçƒ­æœæŠ“å–è„šæœ¬...")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = WeiboHotSearchCrawler()
    
    # æµ‹è¯•è·å–çƒ­æœæ•°æ®
    print("æ­£åœ¨è·å–çƒ­æœæ•°æ®...")
    hot_searches = crawler.get_hot_searches()
    
    if hot_searches:
        print(f"âœ… æˆåŠŸè·å– {len(hot_searches)} æ¡çƒ­æœæ•°æ®")
        
        # æ˜¾ç¤ºå‰5æ¡æ•°æ®
        print("\nå‰5æ¡çƒ­æœæ•°æ®ï¼š")
        for i, item in enumerate(hot_searches[:5]):
            print(f"{i+1}. {item['title']}")
            if item['hot_value']:
                print(f"   çƒ­åº¦: {item['hot_value']}")
        
        # æµ‹è¯•ä¿å­˜åŠŸèƒ½
        print("\næµ‹è¯•ä¿å­˜åŠŸèƒ½...")
        try:
            crawler.save_to_json(hot_searches, 'test_hot_searches.json')
            crawler.save_to_csv(hot_searches, 'test_hot_searches.csv')
            crawler.save_to_txt(hot_searches, 'test_hot_searches.txt')
            print("âœ… æ•°æ®ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥: {e}")
            
    else:
        print("âŒ è·å–çƒ­æœæ•°æ®å¤±è´¥")
        return False
    
    return True

if __name__ == "__main__":
    success = test_crawler()
    if success:
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼Œè„šæœ¬è¿è¡Œæ­£å¸¸ï¼")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è„šæœ¬é…ç½®")